{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danyentezari/bn-ocr/blob/main/keras_ocr_realvalues_bignumber_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obI7x309JZl7"
      },
      "outputs": [],
      "source": [
        "!pip install keras==2.2.5\n",
        "!pip install tensorflow==1.13.1\n",
        "!unzip kaggle.json.zip\n",
        "!mkdir ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download scolianni/mnistasjpg\n",
        "!unzip mnistasjpg.zip\n",
        "!pip install keras==2.2.5\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install 'h5py==2.10.0' --force-reinstall\n",
        "!pip install cairocffi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq_h85uJ60oR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "from scipy.sparse import csr_matrix\n",
        "import cv2\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "import string\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "import itertools\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmvD6G93A82e"
      },
      "outputs": [],
      "source": [
        "alphabet = '0123456789'\n",
        "# def encode_to_labels(txt):\n",
        "#     # encoding each output word into digits\n",
        "#     dig_lst = []\n",
        "#     for index, char in enumerate(txt):\n",
        "#         if char in char_list:\n",
        "#             dig_lst.append(char_list.index(char))\n",
        "#         # else:\n",
        "#         #   print(char)\n",
        "#     return dig_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpM3L7zO8SnO"
      },
      "outputs": [],
      "source": [
        "def labels_to_text(labels, alphabet):\n",
        "    \"\"\"Reverse translation of numerical classes back to characters.\"\"\"\n",
        "    ret = []\n",
        "    for c in labels:\n",
        "        if c == len(alphabet):  # CTC Blank\n",
        "            ret.append(\"\")\n",
        "        else:\n",
        "            ret.append(alphabet[c])\n",
        "    return \"\".join(ret)\n",
        "\n",
        "\n",
        "def text_to_labels(text, alphabet):\n",
        "    \"\"\" Translation of characters to unique integer values\n",
        "    \"\"\"\n",
        "    ret = []\n",
        "    for char in text:\n",
        "        ret.append(alphabet.find(char))\n",
        "    return ret\n",
        "\n",
        "\n",
        "def decode_batch(test_func, word_batch, alphabet):\n",
        "    \"\"\"\n",
        "    - Greedy search -\n",
        "    For a real OCR application, this should be beam\n",
        "    search with a dictionary and language model.\n",
        "    For this example, best path is sufficient.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    prob = []\n",
        "    out = test_func([word_batch])[0]\n",
        "    for i in range(out.shape[0]):\n",
        "        out_best = list(np.argmax(out[i, 2:], 1))\n",
        "        out_prob = np.mean(np.max(out[i, 2:], 1))\n",
        "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
        "        outstr = labels_to_text(out_best, alphabet)\n",
        "        res.append(outstr)\n",
        "        prob.append(out_prob)\n",
        "    return res, prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zafSq78C7JS0",
        "outputId": "9d087aca-070f-4e32-dd68-e76cb1ba4f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq3lRg-x7Jie"
      },
      "outputs": [],
      "source": [
        "# # # Get features\n",
        "# !gdown https://drive.google.com/uc?id=176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ\n",
        "# !unzip formulae.zip\n",
        "\n",
        "# # # Get labels\n",
        "# !gdown https://drive.google.com/uc?id=1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO-6EznR7NK7"
      },
      "outputs": [],
      "source": [
        "# alphabet = string.digits + \\\n",
        "#            string.ascii_lowercase + \\\n",
        "#            string.ascii_uppercase + \\\n",
        "#            '\\n' + \\\n",
        "#            '\\t' + \\\n",
        "#            \"\".join([' ', '~', ':', \"'\", '+', '[', '\\\\', '@', '^', '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '.', '_', '=', ']', '!', '>', ';', '?', '#', '$', ')', '/'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1HL-lIrl_3J"
      },
      "outputs": [],
      "source": [
        "def loadImage(filepath):\n",
        "  img = load_img(filepath)\n",
        "  img = img_to_array(img)\n",
        "  # img = img.reshape(1, 448, 64, 3)\n",
        "  # img = img.reshape(64, 192, 3)\n",
        "  img = img.astype('float32')\n",
        "  img = img / 255.0\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjafOnBskSaP"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "ch = 28\n",
        "cw = 28\n",
        "\n",
        "data_F = np.empty((0, ch, cw, 3))\n",
        "data_L = np.empty((0))\n",
        "\n",
        "i = 0\n",
        "for f_label in listdir('./trainingSet/trainingSet'):\n",
        "  for img_name in listdir(f'./trainingSet/trainingSet/{f_label}'):\n",
        "    img = loadImage(f'./trainingSet/trainingSet/{f_label}/{img_name}')\n",
        "\n",
        "    # if i == 1:\n",
        "    #   break\n",
        "\n",
        "    # print(np.array([img]).shape)\n",
        "    # print(data_F.shape)\n",
        "    # i = 1\n",
        "\n",
        "    data_F = np.append(data_F, [img], axis=0)\n",
        "    data_L = np.append(data_L, [f_label], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdN-6Yc1bZ8u"
      },
      "outputs": [],
      "source": [
        "data_L_dist = to_categorical(data_L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "85_0abkw7Oxa",
        "outputId": "5288f251-d970-4b8b-ed66-a71ac60e9314"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F06DF94D990>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAD5klEQVR4nO1VO0graxCe/feZbFxDHorERoKFB8RKiEgEGyt7I4ggWNiksbG1EsHCSrtUgiJYWajgAyxEGysVxMJCwaAxJiRu9r1ziznZG3NzuNXt7hQh++/833wz880s8DwPAADA87wsy/RIv6qqBq+CP4qitB0CAGMM/tU4jhNFEQAikQgARKNROhcEgSAURSFQQRCCMO3WGorjOMYYx3GEGNyRJKmVviRJsiz/kSPHcUEujLHWvAgXAHp7e1uvqKoauBH9H4AECgCISKC+7xMR27b7+voYY/l8HhFHR0cLhYLjOFdXV8ViEQB4nvd9ny7+sH/GIeN5PpfLGYbx+fmJiJZl2bb9+vparVbX19cHBweJCmOsQwWCo9Z0UqnU5eWl4zie52GLVatVovbw8DA7O0v+mqZRgA5GfQCAWCyWyWQajQYiep5Xr9cdx0FEXdcJmnCLxeLKyoosywBAUumQfiQS6erqAoDJycmLiwsCRURCzOfzuVzu+voaEQ3DsG0bET8+Pubm5jpzDIfDAf9YLLazs2OaJiKWSqXHx8ft7e3V1dXh4WFJkhRF6enpub+/p3iu6x4cHABAd3d3OyjHcYIgBCmcnp5SjvPz82NjY7IsU8igPhsbG7VajXCfnp4mJibaEUVRDHqlqmomk3Fdt1QqeZ6XSCTorSiKhEtzAQALCwuWZZmmaVnW7u5u21Axx3F83ydXXdc1TeN5PpFImKYpiqIoir7vO45D3sHs7+/vS5Lk+z7Hcbqum6b5gynBybJMwzMzM/Py8oKIJycn0NRZOBxu7W8oFAKA4+Nj6uHz83OHmgbZxePxt7c3RKzX64iYTqfbhB0KhYJMp6amdF2n4mqa1uomAAAiMsY8zwOAZDJZrVaj0ahpmkQEmrILh8O1Wg2ao/z+/h4OhwHANE1S69/ZS5Lkui7hlsvlw8NDRVE8z6O5pkxd1xUEgRADGSQSCdu2AUBRFMMw2tMnPypfLpcLBH92dpZMJqEpQ0VRGGO0W9Pp9M3NjWEYjuOsra11WCA8z/M8T2VVVfXu7s4wDMuyEPH8/Hx6evp3pZobmrpPOjUMY2RkpDPN37VgDADGx8dvb29pfSDi0dHR0NBQJpPp7+8fGBj49etXpVJpNBqO43x9fRUKBWguz5aiMhYcMcao9ktLS5VKJcD1PM8wjHK5TOxo8MlhcXGxAyg9B3uP+igIwvLycuvSo4J4nkfDUi6XHccJtl/n1Uddpte0qzRN29vbQ8RGoxFsVd/3SZjf39/ZbBYA4vH4H9MPhEbdIDGnUqlsNru5uRnsJETc2tpKJpOSJAWf2DaR/m//if0F9y0+HgsnqwkAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "sample_image = loadImage('./trainingSet/trainingSet/0/img_1.jpg')\n",
        "Image.fromarray(np.uint8(sample_image*255)).convert('RGB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXCR8ROH7ep6"
      },
      "source": [
        "### Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2nWSEqY7lbC",
        "outputId": "66a8a5b0-0b98-4008-90a0-82574bf39a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 28, 28, 3)\n",
            "(42000,)\n",
            "(42000, 10)\n"
          ]
        }
      ],
      "source": [
        "print(data_F.shape)\n",
        "print(data_L.shape)\n",
        "print(data_L_dist.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI90D4d77n60"
      },
      "outputs": [],
      "source": [
        "# train_L_id = np.array([np.zeros(max_word_length_track)])\n",
        "# train_L_t = []\n",
        "\n",
        "# for i,label in enumerate(train_L):\n",
        "#   ids = np.zeros(max_word_length_track)\n",
        "#   new_item = [float(alphabet_t2id[t]) for i,t in enumerate(list(label))]\n",
        "#   ids[0: len(new_item)] = new_item\n",
        "#   train_L_id = np.append(train_L_id, [ids], axis=0)\n",
        "\n",
        "\n",
        "# # train_L_id = np.delete(train_L_id, [0])\n",
        "# train_L_id = train_L_id[1:]\n",
        "# len(train_L_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3mlAF8Q7ocC"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw-VdKyhZaNE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import codecs\n",
        "import re\n",
        "import datetime\n",
        "import cairocffi as cairo\n",
        "import editdistance\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import pylab\n",
        "import tensorflow\n",
        "from keras import backend as K\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Input, Dense, Activation\n",
        "from keras.layers import Reshape, Lambda\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing import image\n",
        "import keras.callbacks\n",
        "#from keras.metrics import Metric\n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "\n",
        "OUTPUT_DIR = 'image_ocr'\n",
        "\n",
        "\n",
        "def NNModel():\n",
        "  # Input Parameters\n",
        "  img_w = cw\n",
        "  img_h = ch\n",
        "  words_per_epoch = 16000\n",
        "  val_split = 0.2\n",
        "  val_words = int(words_per_epoch * (val_split))\n",
        "\n",
        "  # Network parameters\n",
        "  conv_filters = 16\n",
        "  kernel_size = (3, 3)\n",
        "  pool_size = 2\n",
        "  time_dense_size = 32\n",
        "  rnn_size = 512\n",
        "  minibatch_size = 32\n",
        "\n",
        "\n",
        "  input_shape = (ch, cw, 3)\n",
        "\n",
        "  act = 'relu'\n",
        "  input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
        "  inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
        "                  activation=act, kernel_initializer='he_normal',\n",
        "                  name='conv1')(input_data)\n",
        "  inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n",
        "  inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
        "                  activation=act, kernel_initializer='he_normal',\n",
        "                  name='conv2')(inner)\n",
        "  inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n",
        "\n",
        "  conv_to_rnn_dims = (img_w // (pool_size ** 2),\n",
        "                      (img_h // (pool_size ** 2)) * conv_filters)\n",
        "  inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
        "\n",
        "  # cuts down input size going into RNN:\n",
        "  inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n",
        "\n",
        "  # Two layers of bidirectional GRUs\n",
        "  # GRU seems to work as well, if not better than LSTM:\n",
        "  gru_1 = GRU(rnn_size, return_sequences=True,\n",
        "              kernel_initializer='he_normal', name='gru1')(inner)\n",
        "  gru_1b = GRU(rnn_size, return_sequences=True,\n",
        "                go_backwards=True, kernel_initializer='he_normal',\n",
        "                name='gru1_b')(inner)\n",
        "  gru1_merged = add([gru_1, gru_1b])\n",
        "  gru_2 = GRU(rnn_size, return_sequences=True,\n",
        "              kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
        "  gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True,\n",
        "                kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
        "\n",
        "  # transforms RNN output to character activations:\n",
        "  inner = Dense(\n",
        "      len(alphabet), \n",
        "      kernel_initializer='he_normal',\n",
        "      name='dense2')(concatenate([gru_2, gru_2b])\n",
        "    )\n",
        "  y_pred = Activation('softmax', name='softmax')(inner)\n",
        "\n",
        "\n",
        "\n",
        "  labels = Input(name='the_labels', shape=[1], dtype='float32')\n",
        "  input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "  label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "\n",
        "  # model0 = Model(\n",
        "  #     inputs=[input_data, labels, input_length, label_length],\n",
        "  #     outputs=y_pred\n",
        "  # )\n",
        "\n",
        "\n",
        "  def ctc_lambda_func(args):\n",
        "      y_pred, labels, input_length, label_length = args\n",
        "      # the 2 is critical here since the first couple outputs of the RNN\n",
        "      # tend to be garbage:\n",
        "      y_pred = y_pred[:, 2:, :]\n",
        "      return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "  # Keras doesn't currently support loss funcs with extra parameters\n",
        "  # so CTC loss is implemented in a lambda layer\n",
        "  loss_out = Lambda(\n",
        "      ctc_lambda_func, output_shape=(1,),\n",
        "      name='ctc')([y_pred, labels, input_length, label_length])\n",
        "\n",
        "  # clipnorm seems to speeds up convergence\n",
        "  sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "\n",
        "  model0 = Model(\n",
        "          inputs=[input_data, labels, input_length, label_length],\n",
        "          outputs=loss_out\n",
        "      )\n",
        "\n",
        "  model0.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam')\n",
        "\n",
        "\n",
        "  model0.summary()\n",
        "  train_model = model0\n",
        "\n",
        "  prediction_model = Model(inputs=input_data, outputs=y_pred)\n",
        "\n",
        "  return train_model, prediction_model\n",
        "\n",
        "\n",
        "model0, _ = NNModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaxECOCK8Zr1"
      },
      "outputs": [],
      "source": [
        "# batch_size = 25 #a divisor of training length\n",
        "# epochs = 5\n",
        "\n",
        "# input_length = np.array([ [10] for i in range(len(train_F)) ])\n",
        "# label_length = np.array([ [10] for i in range(len(train_F)) ])\n",
        "\n",
        "# metrics0 = model0.fit(\n",
        "#           x=[\n",
        "#               train_F, \n",
        "#               train_L_dist, \n",
        "#               input_length,\n",
        "#               label_length,\n",
        "#             ], \n",
        "#           # y = np.zeros(len(train_F)),\n",
        "#           y = train_L_dist,\n",
        "#           # batch_size=batch_size, \n",
        "#           epochs = epochs, \n",
        "#           # validation_data = ([val_img, val_padded_txt, val_input_length, val_label_length], val_padded_txt),\n",
        "#           verbose = 2,\n",
        "#           # callbacks = callbacks_list\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhAMGf8UTFgF"
      },
      "outputs": [],
      "source": [
        "# model0.save_weights( './weights1-mnist.h5' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK1qdIDHdyVY"
      },
      "source": [
        "### Alternative Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfOMTtB1hcdF"
      },
      "outputs": [],
      "source": [
        "train_F, test_F, train_L_dist, test_L_dist = train_test_split(data_F, data_L_dist, test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17RoiGw_hsWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59183c1-1b12-4fe2-8e8b-44a2ff794a2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37800, 28, 28, 3), (4200, 28, 28, 3), (37800, 10), (4200, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "train_F.shape, test_F.shape, train_L_dist.shape, test_L_dist.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSttdlwhg4Nx"
      },
      "outputs": [],
      "source": [
        "# With data augmentation to prevent overfitting\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.01, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "#datagen.fit(X_train)\n",
        "train_gen = datagen.flow(train_F, train_L_dist, batch_size=128)\n",
        "test_gen = datagen.flow(test_F, test_L_dist, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "map673iJhIgl"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 128\n",
        "train_steps = train_F.shape[0] // batch_size\n",
        "valid_steps = train_L_dist.shape[0] // batch_size\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_acc\", # metrics to monitor\n",
        "        patience=10, # how many epochs before stop\n",
        "        verbose=1,\n",
        "        mode=\"max\", # we need the maximum accuracy.\n",
        "        restore_best_weights=True, # \n",
        "     )\n",
        "\n",
        "rp = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_acc\",\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        verbose=1,\n",
        "        mode=\"max\",\n",
        "        min_lr=0.00001,\n",
        "     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmORo5YJdqLH"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Lambda, MaxPooling2D # convolution layers\n",
        "from keras.layers import Dense, Dropout, Flatten # core layers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "model1=Sequential()\n",
        "\n",
        "#model1.add(Lambda(standardize,input_shape=(28,28,1)))    \n",
        "model1.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,3)))\n",
        "model1.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
        "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model1.add(BatchNormalization())\n",
        "\n",
        "model1.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model1.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model1.add(BatchNormalization())    \n",
        "\n",
        "model1.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
        "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model1.add(BatchNormalization())\n",
        "    \n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(512,activation=\"relu\"))\n",
        "    \n",
        "model1.add(Dense(10,activation=\"softmax\"))\n",
        "    \n",
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3m3usj0eA-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5882e3fd-498a-4421-9192-87cf883cd297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/5\n",
            "295/295 [==============================] - 710s 2s/step - loss: 0.1600 - acc: 0.9505 - val_loss: 0.1161 - val_acc: 0.9654\n",
            "Epoch 2/5\n",
            "295/295 [==============================] - 597s 2s/step - loss: 0.0625 - acc: 0.9808 - val_loss: 0.0540 - val_acc: 0.9839\n",
            "Epoch 3/5\n",
            "295/295 [==============================] - 561s 2s/step - loss: 0.0471 - acc: 0.9855 - val_loss: 0.0828 - val_acc: 0.9759\n",
            "Epoch 4/5\n",
            "295/295 [==============================] - 524s 2s/step - loss: 0.0450 - acc: 0.9862 - val_loss: 0.0768 - val_acc: 0.9788\n",
            "Epoch 5/5\n",
            "295/295 [==============================] - 532s 2s/step - loss: 0.0368 - acc: 0.9887 - val_loss: 0.0698 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n"
          ]
        }
      ],
      "source": [
        "# metrics1 = model1.fit(x=train_F, y=train_L_dist, epochs=20, verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "metrics1 = model1.fit_generator(train_gen, \n",
        "                              epochs = epochs, \n",
        "                              steps_per_epoch = train_steps,\n",
        "                              validation_data = test_gen,\n",
        "                              validation_steps = valid_steps, \n",
        "                              callbacks=[es, rp])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.save_weights( './weights2-mnist.h5' )"
      ],
      "metadata": {
        "id": "KWIf6CxXvwS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model1.predict(data_F[7300].reshape(1, ch, cw, 3))\n",
        "# np.argmax(yhat, axis=1).tolist()\n",
        "yhat.argmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ8dbCOCwCNI",
        "outputId": "9bcde066-7ef8-4008-9c84-63d1fe326abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Zu6CyARjYj"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics1"
      ],
      "metadata": {
        "id": "IPyIgnd815ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693a1d87-5971-4f05-f490-4cff7593547c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f06d32e6950>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdzBBfKW6Uzy"
      },
      "outputs": [],
      "source": [
        "# pred = model0.predict(\n",
        "#     [train_F, train_L_id, np.array([1]), np.array([1])]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chh_M6jWPUOh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "606d990d-ec7a-4ded-a7a4-03eac25a226a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f06df94d650>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAL2UlEQVR4nO3dX6hl5XnH8e+vxhBivBgrGYaJqWmxpSDBFPFKWktRrDejNxIvyoQETi5iSS5aIiklQiiE0qYXvSidEHH6zxAwVpFStTatgVJxFKujYrRBicM4UxlozE3S6NOLs0aOM/ucfWb/W3vm+X5gs/deZ5+1HtbMb973Xe9e86aqkHTh+4WxC5C0GoZdasKwS00YdqkJwy41YdilJgy71IRh10RJfj3Jvyb53ySvJrlt7Jo0H8OusyT5APAg8DBwGbAB/F2SXx21MM0lfoNOZ0pyNfCfwKU1/AVJ8ijwZFX98ajFaWa27NqtAFePXYRmZ9g1ycvASeAPk1yc5Cbgt4APj1uW5mE3XhMl+STwl2y25keA/wF+WlWfG7Uwzcywa1eS/AdwuKr+euxaNBu78ZooySeTfCjJh5P8AbAPuHfksjQHw67t/B5wnM2x++8AN1bVT8ctSfOwGy81YcsuNWHYpSYMu9SEYZea+MAqD5bEq4HSklVVJm2fq2VPcnOSl4dbIO+aZ1+SlmvmqbckFwE/AG4E3gCeAu6oqhd3+B1bdmnJltGyXwe8WlU/rKqfAd8GDsyxP0lLNE/Y9wM/2vL+jWHb+yTZSHIkyZE5jiVpTku/QFdVh4BDYDdeGtM8Lfsx4Iot7z82bJO0huYJ+1PAVUk+keSDwKeBhxZTlqRFm7kbX1U/T3In8AhwEXBPVb2wsMokLdRK73pzzC4t31K+VCPp/GHYpSYMu9SEYZeaMOxSE4ZdasKwS00YdqkJwy41YdilJgy71IRhl5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmDLvUhGGXmjDsUhOGXWrCsEtNGHapCcMuNWHYpSYMu9SEYZeaMOxSE4ZdamLm9dkBkrwGvA28A/y8qq5dRFGSFm+usA9+u6reWsB+JC2R3XipiXnDXsCjSZ5OsjHpA0k2khxJcmTOY0maQ6pq9l9O9lfVsSQfBR4Dfr+qntjh87MfTNKuVFUmbZ+rZa+qY8PzSeAB4Lp59idpeWYOe5JLklx6+jVwE3B0UYVJWqx5rsbvBR5Icno//1BV/7yQqrQy04Zxw5+vLgBzjdnP+WCO2deOYb/wLGXMLun8YdilJgy71IRhl5ow7FITi7gRRmts2VfbvZp//rBll5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmnGe/AOw0173seW7n0c8ftuxSE4ZdasKwS00YdqkJwy41YdilJgy71ITz7OcB7xnXItiyS00YdqkJwy41YdilJgy71IRhl5ow7FITzrOfB87nefQx77XX+01t2ZPck+RkkqNbtl2W5LEkrwzPe5ZbpqR57aYbfy9w8xnb7gIer6qrgMeH95LW2NSwV9UTwKkzNh8ADg+vDwO3LrguSQs265h9b1UdH16/Cezd7oNJNoCNGY8jaUHmvkBXVZVk26swVXUIOASw0+ckLdesU28nkuwDGJ5PLq4kScswa9gfAg4Orw8CDy6mHEnLkl3cK30fcANwOXAC+Crwj8B3gI8DrwO3V9WZF/Em7ctu/ATer65FqqqJf2Gmhn2RDPtkhl2LtF3Y/bqs1IRhl5ow7FIThl1qwrBLTXiL6xrwavtkzlIsli271IRhl5ow7FIThl1qwrBLTRh2qQnDLjXhPPsKOF88medltWzZpSYMu9SEYZeaMOxSE4ZdasKwS00YdqkJ59lXwPniyTwvq2XLLjVh2KUmDLvUhGGXmjDsUhOGXWrCsEtNOM+uteX97os1tWVPck+Sk0mObtl2d5JjSZ4dHrcst0xJ89pNN/5e4OYJ2/+iqq4ZHv+02LIkLdrUsFfVE8CpFdQiaYnmuUB3Z5Lnhm7+nu0+lGQjyZEkR+Y4lqQ5ZdpFEIAkVwIPV9XVw/u9wFtAAV8D9lXVZ3exn+kHkwZeoJtNVU08MTO17FV1oqreqap3gW8C181TnKTlmynsSfZteXsbcHS7z0paD1Pn2ZPcB9wAXJ7kDeCrwA1JrmGzG/8a8Pkl1tjePN3Z3QzTZt33vPu3G75auxqzL+xgjtln0jXsjtlns9Axu6Tzj2GXmjDsUhOGXWrCsEtNeIvreWCZV53n3fc8v7/KmSDZskttGHapCcMuNWHYpSYMu9SEYZeaMOxSE86zX+DW+c6wZd5Rp7PZsktNGHapCcMuNWHYpSYMu9SEYZeaMOxSE86zazT+77GrZcsuNWHYpSYMu9SEYZeaMOxSE4ZdasKwS01MDXuSK5J8L8mLSV5I8sVh+2VJHkvyyvC8Z/nlSprV1CWbk+wD9lXVM0kuBZ4GbgU+A5yqqq8nuQvYU1VfnrIv/zcCvccv1SzHzEs2V9XxqnpmeP028BKwHzgAHB4+dpjNfwAkralzGrMnuRL4FPAksLeqjg8/ehPYu9DKJC3Urr8bn+QjwP3Al6rqx1u7WFVV23XRk2wAG/MWKmk+U8fsAEkuBh4GHqmqbwzbXgZuqKrjw7j+36rq16bsxzG73uOYfTlmHrNn84x/C3jpdNAHDwEHh9cHgQfnLVLS8uzmavz1wPeB54F3h81fYXPc/h3g48DrwO1VdWrKvmzZpSXbrmXfVTd+UQy7tHwzd+MlXRgMu9SEYZeaMOxSE4ZdasKwS034X0lrNPNO+/oNu3Njyy41YdilJgy71IRhl5ow7FIThl1qwrBLTTjPrtE4T75atuxSE4ZdasKwS00YdqkJwy41YdilJgy71ITz7BqNK8Ksli271IRhl5ow7FIThl1qwrBLTRh2qQnDLjUxNexJrkjyvSQvJnkhyReH7XcnOZbk2eFxy/LL1YUkyY4PLdbU9dmT7AP2VdUzSS4FngZuBW4HflJVf7brg7k+u7R0263PPvUbdFV1HDg+vH47yUvA/sWWJ2nZzmnMnuRK4FPAk8OmO5M8l+SeJHu2+Z2NJEeSHJmrUklzmdqNf++DyUeAfwf+pKq+m2Qv8BZQwNfY7Op/dso+7MZLS7ZdN35XYU9yMfAw8EhVfWPCz68EHq6qq6fsx7BLS7Zd2HdzNT7At4CXtgZ9uHB32m3A0XmLlLQ8u7kafz3wfeB54N1h81eAO4Br2OzGvwZ8friYt9O+bNmlJZurG78ohl1avpm78ZIuDIZdasKwS00YdqkJwy41YdilJgy71IRhl5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmVr1k81vA61veXz5sW0frWtu61gXWNqtF1vZL2/1gpfezn3Xw5EhVXTtaATtY19rWtS6wtlmtqja78VIThl1qYuywHxr5+DtZ19rWtS6wtlmtpLZRx+ySVmfsll3Sihh2qYlRwp7k5iQvJ3k1yV1j1LCdJK8leX5YhnrU9emGNfROJjm6ZdtlSR5L8srwPHGNvZFqW4tlvHdYZnzUczf28ucrH7MnuQj4AXAj8AbwFHBHVb240kK2keQ14NqqGv0LGEl+E/gJ8Denl9ZK8qfAqar6+vAP5Z6q+vKa1HY357iM95Jq226Z8c8w4rlb5PLnsxijZb8OeLWqflhVPwO+DRwYoY61V1VPAKfO2HwAODy8PszmX5aV26a2tVBVx6vqmeH128DpZcZHPXc71LUSY4R9P/CjLe/fYL3Wey/g0SRPJ9kYu5gJ9m5ZZutNYO+YxUwwdRnvVTpjmfG1OXezLH8+Ly/Qne36qvoN4HeBLwzd1bVUm2OwdZo7/SvgV9hcA/A48OdjFjMsM34/8KWq+vHWn4157ibUtZLzNkbYjwFXbHn/sWHbWqiqY8PzSeABNocd6+TE6RV0h+eTI9fznqo6UVXvVNW7wDcZ8dwNy4zfD/x9VX132Dz6uZtU16rO2xhhfwq4KsknknwQ+DTw0Ah1nCXJJcOFE5JcAtzE+i1F/RBwcHh9EHhwxFreZ12W8d5umXFGPnejL39eVSt/ALeweUX+v4E/GqOGber6ZeC/hscLY9cG3Mdmt+7/2Ly28TngF4HHgVeAfwEuW6Pa/pbNpb2fYzNY+0aq7Xo2u+jPAc8Oj1vGPnc71LWS8+bXZaUmvEAnNWHYpSYMu9SEYZeaMOxSE4ZdasKwS038PwWQ/BDl3eMPAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "img = data_F[7300]\n",
        "img = img.astype(np.uint8) \n",
        "plt.title(data_L[7300])\n",
        "plt.imshow(img * 255)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = loadImage('./8.jpg')\n",
        "sample_image = 255 - (sample_image * 255)\n",
        "# Image.fromarray(np.uint8(sample_image*255)).convert('RGB')\n",
        "img = sample_image.astype(np.uint8) \n",
        "plt.imshow(img)\n",
        "\n",
        "yhat = model1.predict(img.reshape(1, ch, cw, 3))\n",
        "# np.argmax(yhat, axis=1).tolist()\n",
        "yhat.argmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "YD3mQE4Kel52",
        "outputId": "42637087-0c1f-469c-fc44-dbc4a7ea224e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSpEH8w27ryb"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/danyentezari/model_weights.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgkTA-AFTW6w"
      },
      "outputs": [],
      "source": [
        "_, model_infer = NNModel()\n",
        "# model_infer.load_weights(\"./model_weights/weights0-mnist.h5\")\n",
        "model_infer.load_weights(\"./weights1-mnist.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uUGXWN1QxYN"
      },
      "outputs": [],
      "source": [
        "# sample = train_F[0].reshape(1, 100, 780, 3)\n",
        "yhat = model_infer.predict(train_F[9000].reshape(1, ch, cw, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VwdmSAp17oE"
      },
      "outputs": [],
      "source": [
        "np.argmax(yhat, axis=1).tolist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "keras-ocr-realvalues-bignumber-mnist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYZtwYfNfIvvzmKQYQWu8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}