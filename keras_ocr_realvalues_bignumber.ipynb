{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-ocr-realvalues-bignumber.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNITvymdd3oHv/G+Ko9+sz9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danyentezari/bn-ocr/blob/main/keras_ocr_realvalues_bignumber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.2.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHCsx9Qpfzb4",
        "outputId": "6f5b6f35-eedb-4480-ada7-9b07aa80f0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.2.5\n",
            "  Downloading Keras-2.2.5-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 24.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.7.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.5) (1.5.2)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires keras<2.9,>=2.8.0rc0, but you have keras 2.2.5 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.2.5 keras-applications-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rBDosdCf1G3",
        "outputId": "28259fe6-a4b8-4279-e481-a78061890daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 65 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.47.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.5.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.1.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'h5py==2.10.0' --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "JOYLHXJ-f283",
        "outputId": "6b8feae5-5ab7-4bb0-95c7-b7d3a6e1dfc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 24.7 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 42.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires keras<2.9,>=2.8.0rc0, but you have keras 2.2.5 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.6 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "dq_h85uJ60oR",
        "outputId": "e41537c5-8b93-48a8-9d3e-288255003ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f42235ad2cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "from scipy.sparse import csr_matrix\n",
        "import cv2\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "import string\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def labels_to_text(labels, alphabet):\n",
        "    \"\"\"Reverse translation of numerical classes back to characters.\"\"\"\n",
        "    ret = []\n",
        "    for c in labels:\n",
        "        if c == len(alphabet):  # CTC Blank\n",
        "            ret.append(\"\")\n",
        "        else:\n",
        "            ret.append(alphabet[c])\n",
        "    return \"\".join(ret)\n",
        "\n",
        "\n",
        "def text_to_labels(text, alphabet):\n",
        "    \"\"\" Translation of characters to unique integer values\n",
        "    \"\"\"\n",
        "    ret = []\n",
        "    for char in text:\n",
        "        ret.append(alphabet.find(char))\n",
        "    return ret\n",
        "\n",
        "\n",
        "def decode_batch(test_func, word_batch, alphabet):\n",
        "    \"\"\"\n",
        "    - Greedy search -\n",
        "    For a real OCR application, this should be beam\n",
        "    search with a dictionary and language model.\n",
        "    For this example, best path is sufficient.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    prob = []\n",
        "    out = test_func([word_batch])[0]\n",
        "    for i in range(out.shape[0]):\n",
        "        out_best = list(np.argmax(out[i, 2:], 1))\n",
        "        out_prob = np.mean(np.max(out[i, 2:], 1))\n",
        "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
        "        outstr = labels_to_text(out_best, alphabet)\n",
        "        res.append(outstr)\n",
        "        prob.append(out_prob)\n",
        "    return res, prob\n"
      ],
      "metadata": {
        "id": "hpM3L7zO8SnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "zafSq78C7JS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get features\n",
        "!gdown https://drive.google.com/uc?id=176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ\n",
        "!unzip formulae.zip\n",
        "\n",
        "# # Get labels\n",
        "!gdown https://drive.google.com/uc?id=1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D"
      ],
      "metadata": {
        "id": "Cq3lRg-x7Jie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = string.digits + \\\n",
        "           string.ascii_lowercase + \\\n",
        "           string.ascii_uppercase + \\\n",
        "           '\\n' + \\\n",
        "           '\\t' + \\\n",
        "           \"\".join([' ', '~', ':', \"'\", '+', '[', '\\\\', '@', '^', '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '.', '_', '=', ']', '!', '>', ';', '?', '#', '$', ')', '/'])"
      ],
      "metadata": {
        "id": "PO-6EznR7NK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(alphabet)"
      ],
      "metadata": {
        "id": "Wq57zqkOhr4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadImage(filepath):\n",
        "  img = load_img(filepath)\n",
        "  img = img_to_array(img)\n",
        "  # img = img.reshape(1, 448, 64, 3)\n",
        "  # img = img.reshape(64, 192, 3)\n",
        "  img = img.astype('float32')\n",
        "  # img = img / 255.0\n",
        "  return img\n",
        "\n",
        "\n",
        "sample_image = loadImage('train/0000003.png')\n",
        "\n",
        "\n",
        "Image.fromarray(np.uint8(sample_image )).convert('RGB')"
      ],
      "metadata": {
        "id": "85_0abkw7Oxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_canvas(canvas_h=300, canvas_w=300):\n",
        "  # Create canvas\n",
        "  canvas_color = 255\n",
        "\n",
        "  pixel_rows = []\n",
        "  row = [canvas_color for x in range(0,canvas_w)]\n",
        "  for i in range(0,canvas_h):\n",
        "    pixel_rows.append(row)\n",
        "  r = pixel_rows\n",
        "  g = pixel_rows\n",
        "  b = pixel_rows\n",
        "\n",
        "  canvas = np.dstack((r,g,b))\n",
        "  return canvas\n",
        "\n",
        "cw = 850\n",
        "ch = 300\n",
        "canvas = create_canvas(ch, cw)\n",
        "\n",
        "def center_pad_image(img, canvas_h=300, canvas_w=300):\n",
        "  # Image positions\n",
        "  img2_width = img.shape[1]\n",
        "  img2_height = img.shape[0]\n",
        "  black_row = np.array([[0 for x in range(0,img2_width)]])\n",
        "  img2 = np.dstack((black_row,black_row,black_row))\n",
        "\n",
        "\n",
        "  startx = int((canvas_w-img2_width)/2)\n",
        "  starty = int((canvas_h-img2_height)/2)\n",
        "  endx = startx + img2_width\n",
        "  endy = starty + img2_height\n",
        "\n",
        "  # Superimpose image\n",
        "  to_superimpose = Image.fromarray(np.uint8(img)).convert('RGB')\n",
        "  # Reset global canvas\n",
        "  canvas[:, :] = 255\n",
        "  canvas[starty:endy, startx:endx] = to_superimpose\n",
        "\n",
        "\n",
        "  return Image.fromarray(np.uint8(canvas)).convert('RGB')\n",
        "\n",
        "\n",
        "center_pad_image(sample_image, ch, cw)"
      ],
      "metadata": {
        "id": "Bdu3KPZL7Qva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_file = open('./math.txt', 'r')\n",
        "label_classes = [label for label in label_file]"
      ],
      "metadata": {
        "id": "-YnVOuHv7UDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mypath = './train'\n",
        "train_filenames = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
      ],
      "metadata": {
        "id": "_euUeNC97VN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features and Labels"
      ],
      "metadata": {
        "id": "jXCR8ROH7ep6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet_list = list(alphabet)\n",
        "alphabet_t2id = dict({})\n",
        "alphabet_id2t = dict({})\n",
        "for i, achar in enumerate(alphabet_list):\n",
        "  alphabet_t2id[achar] = i\n",
        "  alphabet_id2t[i] = achar"
      ],
      "metadata": {
        "id": "crTyPkEs7XLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ch = 100\n",
        "# cw = 720\n",
        "# ch = 300\n",
        "# cw = 780\n",
        "n = 300\n",
        "max_word_length_track = 0\n",
        "\n",
        "train_F = np.empty((n, ch, cw, 3))\n",
        "train_L = ['' for x in range(0,n)]\n",
        "\n",
        "canvas = create_canvas(ch, cw)\n",
        "\n",
        "for i, filname in enumerate(train_filenames[0:n]):\n",
        "  loaded_image = loadImage(f'train/{filname}')\n",
        "  loaded_image = np.array(center_pad_image(loaded_image, ch, cw))\n",
        "\n",
        "  # Normalize\n",
        "  loaded_image = loaded_image / 255\n",
        "\n",
        "  # loaded_image = cv2.resize(loaded_image,(64,64))\n",
        "  train_F[i] = loaded_image\n",
        "  # train_L.append(int(filname.split('.')[0]))\n",
        "  # train_L.append(label_classes[int(filname.split('.')[0])])\n",
        "  train_L[i] = label_classes[int(filname.split('.')[0])]\n",
        "\n",
        "  if len(train_L[i]) > max_word_length_track:\n",
        "    max_word_length_track = len(train_L[i])\n",
        "\n",
        "train_L = np.array(train_L)"
      ],
      "metadata": {
        "id": "5yt47_g87Z6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview training image (centered and padded)\n",
        "Image.fromarray(np.uint8(train_F[0])).convert('RGB')"
      ],
      "metadata": {
        "id": "Fgzh15Dc7h0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_F.shape)\n",
        "print(train_L.shape)"
      ],
      "metadata": {
        "id": "Y2nWSEqY7lbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_L_id = np.array([np.zeros(max_word_length_track)])\n",
        "train_L_t = []\n",
        "\n",
        "for i,label in enumerate(train_L):\n",
        "  ids = np.zeros(max_word_length_track)\n",
        "  new_item = [float(alphabet_t2id[t]) for i,t in enumerate(list(label))]\n",
        "  ids[0: len(new_item)] = new_item\n",
        "  train_L_id = np.append(train_L_id, [ids], axis=0)\n",
        "\n",
        "\n",
        "# train_L_id = np.delete(train_L_id, [0])\n",
        "train_L_id = train_L_id[1:]\n",
        "len(train_L_id)"
      ],
      "metadata": {
        "id": "dI90D4d77n60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "y3mlAF8Q7ocC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cairocffi"
      ],
      "metadata": {
        "id": "_UJoZ_VYfrVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import itertools\n",
        "import codecs\n",
        "import re\n",
        "import datetime\n",
        "import cairocffi as cairo\n",
        "import editdistance\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import pylab\n",
        "import tensorflow\n",
        "from keras import backend as K\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Input, Dense, Activation\n",
        "from keras.layers import Reshape, Lambda\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing import image\n",
        "import keras.callbacks\n",
        "#from keras.metrics import Metric\n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "\n",
        "OUTPUT_DIR = 'image_ocr'\n",
        "\n",
        "\n",
        "def NNModel():\n",
        "  # Input Parameters\n",
        "  img_w = cw\n",
        "  img_h = ch\n",
        "  words_per_epoch = 16000\n",
        "  val_split = 0.2\n",
        "  val_words = int(words_per_epoch * (val_split))\n",
        "\n",
        "  # Network parameters\n",
        "  conv_filters = 16\n",
        "  kernel_size = (3, 3)\n",
        "  pool_size = 2\n",
        "  time_dense_size = 32\n",
        "  rnn_size = 512\n",
        "  minibatch_size = 32\n",
        "\n",
        "\n",
        "  input_shape = (None, None, 3)\n",
        "\n",
        "  act = 'relu'\n",
        "  input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
        "  inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
        "                  activation=act, kernel_initializer='he_normal',\n",
        "                  name='conv1')(input_data)\n",
        "  inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n",
        "  inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
        "                  activation=act, kernel_initializer='he_normal',\n",
        "                  name='conv2')(inner)\n",
        "  inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n",
        "\n",
        "  conv_to_rnn_dims = (img_w // (pool_size ** 2),\n",
        "                      (img_h // (pool_size ** 2)) * conv_filters)\n",
        "  inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
        "\n",
        "  # cuts down input size going into RNN:\n",
        "  inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n",
        "\n",
        "  # Two layers of bidirectional GRUs\n",
        "  # GRU seems to work as well, if not better than LSTM:\n",
        "  gru_1 = GRU(rnn_size, return_sequences=True,\n",
        "              kernel_initializer='he_normal', name='gru1')(inner)\n",
        "  gru_1b = GRU(rnn_size, return_sequences=True,\n",
        "                go_backwards=True, kernel_initializer='he_normal',\n",
        "                name='gru1_b')(inner)\n",
        "  gru1_merged = add([gru_1, gru_1b])\n",
        "  gru_2 = GRU(rnn_size, return_sequences=True,\n",
        "              kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
        "  gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True,\n",
        "                kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
        "\n",
        "  # transforms RNN output to character activations:\n",
        "  inner = Dense(20, kernel_initializer='he_normal', name='dense2')(concatenate([gru_2, gru_2b]))\n",
        "  y_pred = Activation('softmax', name='softmax')(inner)\n",
        "\n",
        "\n",
        "\n",
        "  labels = Input(name='the_labels', shape=[max_word_length_track], dtype='float32')\n",
        "  input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "  label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "\n",
        "  # model0 = Model(\n",
        "  #     inputs=[input_data, labels, input_length, label_length],\n",
        "  #     outputs=y_pred\n",
        "  # )\n",
        "\n",
        "\n",
        "  def ctc_lambda_func(args):\n",
        "      y_pred, labels, input_length, label_length = args\n",
        "      # the 2 is critical here since the first couple outputs of the RNN\n",
        "      # tend to be garbage:\n",
        "      y_pred = y_pred[:, 2:, :]\n",
        "      return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "  # Keras doesn't currently support loss funcs with extra parameters\n",
        "  # so CTC loss is implemented in a lambda layer\n",
        "  loss_out = Lambda(\n",
        "      ctc_lambda_func, output_shape=(1,),\n",
        "      name='ctc')([y_pred, labels, input_length, label_length])\n",
        "\n",
        "  # clipnorm seems to speeds up convergence\n",
        "  sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "\n",
        "  model0 = Model(\n",
        "          inputs=[input_data, labels, input_length, label_length],\n",
        "          outputs=loss_out\n",
        "      )\n",
        "\n",
        "  model0.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam')\n",
        "\n",
        "\n",
        "  model0.summary()\n",
        "  train_model = model0\n",
        "\n",
        "  prediction_model = Model(inputs=input_data, outputs=y_pred)\n",
        "\n",
        "  return train_model, prediction_model\n",
        "\n",
        "\n",
        "model0, _ = NNModel()"
      ],
      "metadata": {
        "id": "Jw-VdKyhZaNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_L_id[0])"
      ],
      "metadata": {
        "id": "RovqYerU7Bjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_word_length_track"
      ],
      "metadata": {
        "id": "2Q1GcD_Pm9LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 25 #a divisor of training length\n",
        "epochs = 20\n",
        "\n",
        "input_length = np.array([ [1] for i in range(n) ])\n",
        "label_length = np.array([ [1] for i in range(n) ])\n",
        "\n",
        "metrics0 = model0.fit(\n",
        "          x=[\n",
        "              train_F, \n",
        "              train_L_id, \n",
        "              input_length,\n",
        "              label_length,\n",
        "            ], \n",
        "          y = np.zeros(len(train_F)),\n",
        "          # batch_size=batch_size, \n",
        "          epochs = epochs, \n",
        "          # validation_data = ([val_img, val_padded_txt, val_input_length, val_label_length], val_padded_txt),\n",
        "          # verbose = 2,\n",
        "          # callbacks = callbacks_list\n",
        ")"
      ],
      "metadata": {
        "id": "YaxECOCK8Zr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0.save_weights( './weights2.h5' )"
      ],
      "metadata": {
        "id": "KhAMGf8UTFgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[print(i.shape, i.dtype) for i in model0.inputs]\n",
        "# [print(o.shape, o.dtype) for o in model0.outputs]\n",
        "# [print(l.name, l.input_shape, l.dtype) for l in model0.layers]"
      ],
      "metadata": {
        "id": "qnxofPy98_qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "h6Zu6CyARjYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pred = model0.predict(\n",
        "#     [train_F, train_L_id, np.array([1]), np.array([1])]\n",
        "# )"
      ],
      "metadata": {
        "id": "ZdzBBfKW6Uzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred"
      ],
      "metadata": {
        "id": "J38SSroVO0bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "img = train_F[0]\n",
        "img = img.astype(np.uint8) \n",
        "plt.title(train_L[0])\n",
        "plt.imshow(img * 255)"
      ],
      "metadata": {
        "id": "chh_M6jWPUOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, model_infer = NNModel()\n",
        "model_infer.load_weights(\"./weights2.h5\")"
      ],
      "metadata": {
        "id": "rgkTA-AFTW6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_infer.summary()"
      ],
      "metadata": {
        "id": "cv-kBTc4UwCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_F)"
      ],
      "metadata": {
        "id": "FP3Lr2tI1uBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample = train_F[0].reshape(1, 100, 780, 3)\n",
        "yhat = model_infer.predict(train_F[0].reshape(1, ch, cw, 3))"
      ],
      "metadata": {
        "id": "5uUGXWN1QxYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat.shape"
      ],
      "metadata": {
        "id": "_VwdmSAp17oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(yhat)"
      ],
      "metadata": {
        "id": "MHQOoW--SN7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat[0][211][10]"
      ],
      "metadata": {
        "id": "B6SusVmf5-1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(yhat, axis=1)"
      ],
      "metadata": {
        "id": "lfsL9qqVRQL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(alphabet)"
      ],
      "metadata": {
        "id": "ypPBYoMLSkZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, y_ in enumerate(yhat[0]):\n",
        "  print(y_)"
      ],
      "metadata": {
        "id": "wBg2hQcKh3aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_y9zMnCJk1cm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}